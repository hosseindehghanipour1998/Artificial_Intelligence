{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In his Great name\n",
    "\n",
    "#### Artificial Intelligence - Regressions\n",
    "<hr/>\n",
    "\n",
    "\n",
    "## Team Members\n",
    "| Name (Alphabetically ordered)| StdNo |\n",
    "| ----------- | ----------- |\n",
    "| Sam Asadi | 9532287 |\n",
    "| Hossein Dehghanipour | 9532250 |\n",
    "| Bahare Moradi | 9532245 |\n",
    "\n",
    "> Shiraz University - Spring 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report No. 3\n",
    "### Logistic Regression\n",
    "<hr/>\n",
    "\n",
    "> Written By : **Hossein Dehghanipour-9532250**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main purpose of Q3\n",
    "The main purpose of question 3 was to have the students get familiar with the basis of logistic regression and it's apllications beside the benefit of learnign the methods of implementation.\n",
    "\n",
    "As the question ___had not mentioned___ neighter limited us in the usage of methods of _theta_ calculation, I personally prefered to use the _gradient_ as the core of _theta computation_ and I'll discuss the details with you in a few later lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "The first and most important part of a code is using the available libraries and code snippets as wisely as possible. We'd better only import the libraries that we need or we may face some performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ExtraLibraries.Writer import Writer as Writer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Handling\n",
    "In order to be able to maintain and edit the code as easily as possible, we'd better to put all of our crucial variable( which we may change a lot during the process of coding ) in a class which is accessible by all modules and elements outside the \"main\" moudule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommandCenter :\n",
    "    TEST_SIZE = 0.2\n",
    "    TRAIN_SIZE = 1 - TEST_SIZE\n",
    "    BOUNDARY_PROBABILTY = 0.5\n",
    "    PLOT_SCALER = 0.5\n",
    "\n",
    "\n",
    "class DataCenter :\n",
    "    X_train = None\n",
    "    Y_train = None\n",
    "    X_test  = None\n",
    "    Y_test = None\n",
    "    Thetas = None\n",
    "    FILE_NAME = \"iris.csv\"\n",
    "    X = None\n",
    "    Y = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Functions\n",
    "The somplication of A.I and Logistic Regression will be gone if we try to code each and every part of the program functionally as the golden rule of programming says \n",
    ">Divide and Conquer, Then solve the problem piece by piece.\n",
    "\n",
    "Here you will see some basic and important functions that are being used a lot during the program and process of bulidng a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def hypothesis(X, Thetas):\n",
    "    z = np.dot(X, Thetas)\n",
    "    d = sigmoid(z)\n",
    "    return d\n",
    "\n",
    "\n",
    "def costFunction(X, Y, Thetas):\n",
    "    m = len(Y)\n",
    "    h = hypothesis(X, Thetas)\n",
    "    cost = Y * np.log(h) + (1 - Y) * np.log(1 - h)\n",
    "    return cost.sum() / m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theta Update\n",
    "The main goal of our problem is to reach the best curve that would fit both our test set and train set. In order to gain such a goal , we must calculate the __derivation__ of our __Cost Function__ and then find the _most optimal_ thetas possible. If we calculate the equation and set it equal to zero, by solving tthe equation, we would reach such a formula ( _which is vectorized_ ) and the rest of the process would be a piece of cake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateThetas(X, Y, Thetas, alpha ):\n",
    "    m = len(X)\n",
    "    h = hypothesis(X, Thetas)\n",
    "    gradient = (np.dot(X.T, h - Y) / m) * alpha\n",
    "    Thetas = Thetas - gradient\n",
    "    return Thetas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula above can be gained by solving the equation which is caused by derivation of the cost function. The description of the solution could be found on _Youtube_ or obtained by the descriptions mentioned by ___Dr.Andrew NG - MIT University___. The description of how we have reached this formula is ___Up To You___. Feel free to search and use __Google_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core\n",
    "The most important part of a machine is training it. Fortunately, this part of our problem is not a big deal here due to the simplicity of the problem and not the essence of having a ___Multi Layered Nueral Network___. So the trainig part will be summarized in a few lines of code. But remeber, in order to gain a __higher accuracy__ we would iterate over the training process for multiple times. ( maybe 10 or 100 of thousands of time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, Thetas, alpha, iters):\n",
    "    costLog = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        Thetas = updateThetas(X, Y, Thetas, alpha)\n",
    "        cost = costFunction(X, Y, Thetas)\n",
    "        costLog.append(cost)\n",
    "\n",
    "    return Thetas, costLog\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "One the demands of the question was to calculate the accuracy of the algorithm and report it to the user. In the few lines of the code below, you can see that the accuracy has been calculated and logged in a varibale called `accuracy` which will be shown to the user later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_accuracy():\n",
    "    \n",
    "    h = hypothesis(DataCenter.X_test,DataCenter.Thetas)\n",
    "    def decisionBoundry(h):\n",
    "        h1 = []\n",
    "        for i in h:\n",
    "            if ( i >= CommandCenter.BOUNDARY_PROBABILTY).any():\n",
    "                h1.append(1)\n",
    "            else:\n",
    "                h1.append(0)\n",
    "        return h1\n",
    "    y_pred = decisionBoundry(h)\n",
    "    accuracy = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        if (y_pred[i] == DataCenter.Y_test[i]).any():\n",
    "            accuracy += 1\n",
    "    return (accuracy/len(DataCenter.Y_test)* 100) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "I would vogorously say that the most important part of an AI problem or data science algorithm, is the _Data Processing_ part.\n",
    "In order to gain the best results, we must ignore the corrupted data or _reform_ them is possible. Fortunately the presented data set was _Nan_ free and did not have any missing data. So my approach would be easier and less complicated than working with most of other data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(fileName):\n",
    "    dataSet = pd.read_csv(fileName)\n",
    "    dataSet = dataSet[dataSet.Result != \"Iris-versicolor\"]\n",
    "    #dataSet = dataSet[dataSet.Result != \"Iris-virginica\"]\n",
    "    dataSet[\"Result\"] = dataSet[\"Result\"].apply(lambda x : 1 if x == \"Iris-setosa\" else 0 )\n",
    "    \n",
    "    # Create X (all the feature columns)\n",
    "    X = dataSet.drop(\"Result\", axis=1)\n",
    "    DataCenter.X = X\n",
    "\n",
    "    # Create y (the target column)\n",
    "    Y = dataSet[\"Result\"]\n",
    "    DataCenter.Y = Y\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=CommandCenter.TEST_SIZE)\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    \n",
    "    # Add Bias to our Features Matrix\n",
    "    X_train = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "    X_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "     Here, our X is a two-dimensional array and y is a one-dimensional array. \n",
    "     Let’s make the ‘y’ two-dimensional to match the dimensions.\n",
    "    '''\n",
    "    Y_train = Y_train[:, np.newaxis]\n",
    "    Y_test = Y_test[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    return X_train , Y_train , X_test , Y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotter\n",
    "In order to customize the calculated data , I would write a plotter class of my own. This piece of code shown blow doesn't require any technical explanations and only demands your _patience_ and _familiarity_ with the syntax of Python which I'm sure you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter :\n",
    "\n",
    "    def saveFig(plotPointer , figName , figNumbPath ):\n",
    "        filerIO = Writer(figNumbPath)\n",
    "        lines = filerIO.readFile()\n",
    "        if ( len(lines) == 0 ):\n",
    "            filerIO.append(1) \n",
    "            lines[0] = 1 \n",
    "        plotPointer.savefig( str(figName) + str(lines[0]) + '.png')\n",
    "        filerIO.clearFile()\n",
    "        filerIO.append(str(int(lines[0]) + 1) )   \n",
    "        print(\"Figure saved in : \" + str(figName))\n",
    "      \n",
    "    def plotter(plotInf, X0set , X1set , ResultSet , path ):\n",
    "        scaler = CommandCenter.PLOT_SCALER\n",
    "        X0 = ( X0set )\n",
    "        weights = DataCenter.Thetas\n",
    "        X1 = ( X1set )\n",
    "\n",
    "        class_A = []\n",
    "        class_B = []\n",
    "\n",
    "        for i in range(len( ResultSet)) :\n",
    "            if(ResultSet[i] == 0) :\n",
    "                class_A.append(i)\n",
    "            else :\n",
    "                class_B.append(i)\n",
    "        \n",
    "        # getting the x co-ordinates of the decision boundary\n",
    "        x_cords = np.array([min(X0) - scaler, max(X0) + scaler])\n",
    "        y_cords = (weights[1] * x_cords + weights[0]) * (-1 / weights[2])\n",
    "        # plot class A data with red color\n",
    "        plt.scatter([X0[i] for i in class_A], [X1[i] for i in class_A], color='blue',  label='Iris-virginica')\n",
    "        # plot class B data with red color\n",
    "        plt.scatter([X0[i] for i in class_B], [X1[i] for i in class_B], color='red', label='Iris-setosa')\n",
    "        \n",
    "        # plot class B data with red color\n",
    "        plt.plot(x_cords, y_cords, color = 'purple', label=\"Boundary\")\n",
    "        plt.legend()\n",
    "        \n",
    " \n",
    "\n",
    "        plt.xlabel('X0 - axis') \n",
    "        plt.ylabel('X1 - axis') \n",
    "        plt.title(\" Logistic Regression : \" + str(plotInf))  \n",
    "\n",
    "        plt.legend()         \n",
    "        Plotter.saveFig(plt , path + \"/logistic\" , \"Plots/figureNumber.txt\" )\n",
    "        plt.show()\n",
    "    # creates a new folder for each drawn plot.\n",
    "    def createNewDirectory(foldNumbPath):\n",
    "        import os\n",
    "        filerIO = Writer(foldNumbPath)\n",
    "        lines = filerIO.readFile()\n",
    "        if ( len(lines) == 0 ):\n",
    "            filerIO.append(1)        \n",
    "            lines[0] = 1\n",
    "        os.mkdir( \"Plots/Iteration-\" +str(lines[0] ))   \n",
    "        path = \"Plots/Iteration-\" + str(lines[0]) \n",
    "        filerIO.clearFile()\n",
    "        filerIO.append(str(int(lines[0]) + 1) ) \n",
    "        return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "The `main` part of the code, is runnig the functions and showing the results to the user. These actions are being done in the snippet code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showInfo(X_train,X_test, accuracy ,history):\n",
    "    \n",
    "    trainingSetLenght = len(X_train)/(len(X_train) + len(X_test)) * 100\n",
    "    testSetLenght = len(X_test)/(len(X_train) + len(X_test)) * 100\n",
    "    print(\"Training Set Length : %d Percent \" %  trainingSetLenght)\n",
    "    print(\"Test Set  Length : %d percent\" % testSetLenght )\n",
    "    print(\"Test Set  Accuracy : %d percent\" %accuracy)\n",
    "    for i in range(len(history)):\n",
    "        if(i % 10000 == 0 ):\n",
    "            print(\"Error : %d | ItNo : %d\" %(history[i],i))\n",
    "\n",
    "    #S = \"Training Set Length :\" + str(trainingSetLenght) + \" Percent \\n\" + \"Test Set  Length : \"+ str(testSetLenght)+\" Percent \\n\"  + \"Test Set  Accuracy :\"+str(accuracy)+\" percent \\n\" \n",
    "    S = \"TestSet : Accuracy :\"+str(accuracy) + \"%\"\n",
    "    return S\n",
    "        \n",
    "def main() :\n",
    "    print(\"Processing ... Please Wait.\")\n",
    "    DataCenter.X_train , DataCenter.Y_train , DataCenter.X_test , DataCenter.Y_test = ReadData(DataCenter.FILE_NAME)\n",
    "    #initialize the Theta Matrix\n",
    "    initialThetas = np.zeros((3,1))\n",
    "    # Train the model\n",
    "    DataCenter.Thetas, history = train(DataCenter.X_train, DataCenter.Y_train, initialThetas, 0.1, 100000)\n",
    "    # plot the model\n",
    "    plotInf = showInfo(DataCenter.X_train,DataCenter.X_test, calculate_accuracy(),history )\n",
    "    path  = Plotter.createNewDirectory(\"Plots/folderNumber.txt\")\n",
    "    Plotter.plotter(\"TrainSet\" , DataCenter.X_train[:,1] , DataCenter.X_train[:,2] , DataCenter.Y_train ,path )\n",
    "    Plotter.plotter(plotInf , DataCenter.X_test[:,1] , DataCenter.X_test[:,2] , DataCenter.Y_test ,path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
