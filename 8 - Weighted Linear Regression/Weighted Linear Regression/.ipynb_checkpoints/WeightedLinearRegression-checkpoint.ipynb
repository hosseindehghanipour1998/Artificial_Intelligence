{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial Intelligence - Regressions\n",
    "<hr/>\n",
    "\n",
    "\n",
    "## Team Members\n",
    "| Name (Alphabetically ordered)| StdNo |\n",
    "| ----------- | ----------- |\n",
    "| Sam Asadi | 9532287 |\n",
    "| Hossein Dehghanipour | 9532250 |\n",
    "| Bahare Moradi | 9532245 |\n",
    "\n",
    "Shiraz University - Spring 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted the number of page views for a given page index using a hypothesis $h_\\theta(x)$ defined as :\n",
    "$$ h_\\theta(x) = \\theta_0 x_0 + \\theta_1 x_1 $$\n",
    "$$ = \\sum_{i=0}^{n-1} \\theta_i x_i $$\n",
    "$$ = \\theta^T x $$\n",
    "\n",
    "Goal is to find the parameter vector $\\theta$ which minimizes the square of the error between the predicted value $h_\\theta(x)$ and the actual output $y$ for all $j$ values in the training set with weight $w^j$ i.e.\n",
    "\n",
    "$$ min \\sum_{j=1}^{m}w^j[h_\\theta(x^j)-y^j]^2 $$\n",
    "\n",
    "where,\n",
    "\n",
    "#### W is the diagonal matrix of dimension [m x m].\n",
    "$$ W = \n",
    " \\begin{bmatrix}\n",
    "  w^0 & 0 & \\cdots & 0 \\\\\n",
    "  0 & w^1 & \\cdots & 0 \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  0 & 0 & \\cdots & w^m \n",
    " \\end{bmatrix} $$\n",
    " \n",
    "#### X is the input sequence of dimension [m x n]\n",
    "\n",
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "  x_0^1 & x_1^1 \\\\\n",
    "  x_0^2 & x_1^2 \\\\\n",
    "  \\vdots  & \\vdots \\\\\n",
    "  x_0^m & x_1^m \n",
    " \\end{bmatrix} $$\n",
    " \n",
    "#### Y is the measured values of dimension [m x 1]\n",
    "\n",
    "$$ Y = \n",
    "\\begin{bmatrix}\n",
    "  y^1 \\\\\n",
    "  y^2 \\\\\n",
    "  \\vdots \\\\\n",
    "  y^m\n",
    " \\end{bmatrix} $$\n",
    " \n",
    "#### $\\theta$ is the parameter vector of dimension [n x 1].\n",
    "\n",
    "$$ \\theta = \n",
    "\\begin{bmatrix}\n",
    "  \\theta_0 \\\\\n",
    "  \\theta_1\n",
    " \\end{bmatrix} $$\n",
    " \n",
    " Defining the cost function $\\theta(j)$ as,\n",
    " \n",
    " $$ J(\\theta) = \\frac{1}{2} \\sum_{j=1}^{m} w^j [h_\\theta(x^j)-y^j]^2 = \\frac{1}{2}(X\\theta-Y)^T W(X\\theta-Y) $$\n",
    " \n",
    " To find the value of $\\theta$ which minimizes $J(\\theta)$, we can differentiate $J(\\theta)$ with respect to $\\theta$, i.e.\n",
    " \n",
    " $$ \\frac{\\partial}{\\partial \\theta} J(\\theta) = (X^T W X \\theta - X^T WY) $$\n",
    " \n",
    " To find the value of $\\theta$ which minimizes $\\theta$,  we set\n",
    " \n",
    " $$ \\frac{\\partial}{\\partial \\theta} J(\\theta) = 0 $$\n",
    " $$ (X^T W X \\theta - X^T WY) = 0 $$\n",
    " \n",
    " The weighted least squares solution is, \n",
    " \n",
    " $$ \\theta = (X^T WX)^{-1} X^TWY $$\n",
    " \n",
    " A fairly standard choice for the weights is\n",
    " \n",
    " $$ w^{(i)} = \\exp(-\\frac{(x^i - x)^2}{2}) $$\n",
    " \n",
    " > Here, the $w^{(i)}$’s are non-negative valued weights. Intuitively, if $w^{(i)}$is large for a particular value of $i$, then in picking $\\theta$, we’ll try hard to make $ (y^{(i)} - \\theta^T x^{(i)})^2 $ small. Ifw(i)is small, then the $ (y^{(i)} - \\theta^T x^{(i)})^2 $ error term will bepretty much ignored in the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
